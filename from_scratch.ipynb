{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "from random import randint\n",
    "import idx2numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filex = 't10k-images-idx3-ubyte'\n",
    "filey = 't10k-labels-idx1-ubyte'\n",
    "X = idx2numpy.convert_from_file(filex)\n",
    "y = idx2numpy.convert_from_file(filey)\n",
    "X = X / 255\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "X.shape,y.shape\n",
    "\n",
    "x_train = X[:4000]\n",
    "x_test  = X[4000:]\n",
    "y_train = y[:4000]\n",
    "y_test  = y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    first_layer = {}\n",
    "    second_layer = {}\n",
    "\n",
    "    def __init__(self, inputs, hidden, outputs):\n",
    "        self.first_layer['para'] = np.random.randn(hidden,inputs) / np.sqrt(num_inputs)\n",
    "        self.first_layer['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden)\n",
    "        self.second_layer['para'] = np.random.randn(outputs,hidden) / np.sqrt(hidden)\n",
    "        self.second_layer['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden)\n",
    "        self.input_size = inputs\n",
    "        self.hid_size = hidden\n",
    "        self.output_size = outputs\n",
    "\n",
    "    def __activfunc(self,Z,type = 'ReLU',deri = False):\n",
    "        if type == 'ReLU':\n",
    "            if deri == True:\n",
    "                return np.array([1 if i>0 else 0 for i in np.squeeze(Z)])\n",
    "            else:\n",
    "                return np.array([i if i>0 else 0 for i in np.squeeze(Z)])\n",
    "        elif type == 'Sigmoid':\n",
    "            if deri == True:\n",
    "                return 1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
    "            else:\n",
    "                return 1/(1+np.exp(-Z))\n",
    "        elif type == 'tanh':\n",
    "            if deri == True:\n",
    "                return \n",
    "            else:\n",
    "                return 1-(np.tanh(Z))**2\n",
    "        else:\n",
    "            raise TypeError('Invalid type!')\n",
    "\n",
    "    def __Softmax(self,z):\n",
    "        return 1/sum(np.exp(z)) * np.exp(z)\n",
    "\n",
    "    def __cross_entropy_error(self,v,y):\n",
    "        return -np.log(v[y])\n",
    "\n",
    "    def __forward(self,x,y):\n",
    "        Z = np.matmul(self.first_layer['para'],x).reshape((self.hid_size,1)) + self.first_layer['bias']\n",
    "        H = np.array(self.__activfunc(Z)).reshape((self.hid_size,1))\n",
    "        U = np.matmul(self.second_layer['para'],H).reshape((self.output_size,1)) + self.second_layer['bias']\n",
    "        predict_list = np.squeeze(self.__Softmax(U))\n",
    "        error = self.__cross_entropy_error(predict_list,y)\n",
    "        \n",
    "        dic = {\n",
    "            'Z':Z,\n",
    "            'H':H,\n",
    "            'U':U,\n",
    "            'f_X':predict_list.reshape((1,self.output_size)),\n",
    "            'error':error\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def __back_propagation(self,x,y,f_result):\n",
    "        E = np.array([0]*self.output_size).reshape((1,self.output_size))\n",
    "        E[0][y] = 1\n",
    "        dU = (-(E - f_result['f_X'])).reshape((self.output_size,1))\n",
    "        db_2 = copy.copy(dU)\n",
    "        dC = np.matmul(dU,f_result['H'].transpose())\n",
    "        delta = np.matmul(self.second_layer['para'].transpose(),dU)\n",
    "        db_1 = delta.reshape(self.hid_size,1)*self.__activfunc(f_result['Z'],deri=True).reshape(self.hid_size,1)\n",
    "        dW = np.matmul(db_1.reshape((self.hid_size,1)),x.reshape((1,784)))\n",
    "\n",
    "        grad = {\n",
    "            'dC':dC,\n",
    "            'db_2':db_2,\n",
    "            'db_1':db_1,\n",
    "            'dW':dW\n",
    "        }\n",
    "        return grad\n",
    "\n",
    "    def __optimize(self,b_result, learning_rate):\n",
    "        self.second_layer['para'] -= learning_rate*b_result['dC']\n",
    "        self.second_layer['bias'] -= learning_rate*b_result['db_2']\n",
    "        self.first_layer['bias'] -= learning_rate*b_result['db_1']\n",
    "        self.first_layer['para'] -= learning_rate*b_result['dW']\n",
    "\n",
    "    def __loss(self,X_train,Y_train):\n",
    "        loss = 0\n",
    "        for n in range(len(X_train)):\n",
    "            y = Y_train[n]\n",
    "            x = X_train[n][:]\n",
    "            loss += self.__forward(x,y)['error']\n",
    "        return loss\n",
    "\n",
    "    def train(self, X_train, Y_train, num_iterations = 1000, learning_rate = 0.5):\n",
    "        rand_indices = np.random.choice(len(X_train), num_iterations, replace=True)\n",
    "        \n",
    "        def l_rate(base_rate, ite, num_iterations, schedule = False):\n",
    "            if schedule == True:\n",
    "                return base_rate * 10 ** (-np.floor(ite/num_iterations*5))\n",
    "            else:\n",
    "                return base_rate\n",
    "\n",
    "        count = 1\n",
    "        loss_dict = {}\n",
    "        test_dict = {}\n",
    "\n",
    "        for i in rand_indices:\n",
    "            f_result = self.__forward(X_train[i],Y_train[i])\n",
    "            b_result = self.__back_propagation(X_train[i],Y_train[i],f_result)\n",
    "            self.__optimize(b_result,l_rate(learning_rate,i,num_iterations,True))\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                if count % 5000 == 0:\n",
    "                    loss = self.__loss(X_train,Y_train)\n",
    "                    test = self.testing(x_test,y_test)\n",
    "                    print('Trained for {} times,'.format(count),'loss = {}, test = {}'.format(loss,test))\n",
    "                    loss_dict[str(count)]=loss\n",
    "                    test_dict[str(count)]=test\n",
    "                else:\n",
    "                    print('Trained for {} times,'.format(count))\n",
    "            count += 1\n",
    "\n",
    "        print('Training finished!')\n",
    "        return loss_dict, test_dict\n",
    "\n",
    "    def testing(self,X_test, Y_test):\n",
    "        total_correct = 0\n",
    "        for n in range(len(X_test)):\n",
    "            y = Y_test[n]\n",
    "            x = X_test[n][:]\n",
    "            prediction = np.argmax(self.__forward(x,y)['f_X'])\n",
    "            if (prediction == y):\n",
    "                total_correct += 1\n",
    "        print('Accuarcy Test: ',total_correct/len(X_test))\n",
    "        return total_correct/np.float(len(X_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 1000 times,\n",
      "Trained for 2000 times,\n",
      "Trained for 3000 times,\n",
      "Trained for 4000 times,\n",
      "Accuarcy Test:  0.874\n",
      "Trained for 5000 times, loss = 1186.134098459381, test = 0.874\n",
      "Trained for 6000 times,\n",
      "Trained for 7000 times,\n",
      "Trained for 8000 times,\n",
      "Trained for 9000 times,\n",
      "Accuarcy Test:  0.9141666666666667\n",
      "Trained for 10000 times, loss = 605.89103177157, test = 0.9141666666666667\n",
      "Trained for 11000 times,\n",
      "Trained for 12000 times,\n",
      "Trained for 13000 times,\n",
      "Trained for 14000 times,\n",
      "Accuarcy Test:  0.9055\n",
      "Trained for 15000 times, loss = 505.1772751179061, test = 0.9055\n",
      "Trained for 16000 times,\n",
      "Trained for 17000 times,\n",
      "Trained for 18000 times,\n",
      "Trained for 19000 times,\n",
      "Accuarcy Test:  0.9296666666666666\n",
      "Trained for 20000 times, loss = 278.25717244173484, test = 0.9296666666666666\n",
      "Trained for 21000 times,\n",
      "Trained for 22000 times,\n",
      "Trained for 23000 times,\n",
      "Trained for 24000 times,\n",
      "Accuarcy Test:  0.9346666666666666\n",
      "Trained for 25000 times, loss = 201.66461442308562, test = 0.9346666666666666\n",
      "Trained for 26000 times,\n",
      "Trained for 27000 times,\n",
      "Trained for 28000 times,\n",
      "Trained for 29000 times,\n",
      "Accuarcy Test:  0.9261666666666667\n",
      "Trained for 30000 times, loss = 138.22349725832996, test = 0.9261666666666667\n",
      "Training finished!\n",
      "Accuarcy Test:  0.9261666666666667\n"
     ]
    }
   ],
   "source": [
    "# set the number of iterations\n",
    "num_iterations = 30000\n",
    "# set the base learning rate\n",
    "learning_rate = 0.01\n",
    "# number of inputs\n",
    "num_inputs = 28*28\n",
    "# number of outputs\n",
    "num_outputs = 10\n",
    "# size of hidden layer\n",
    "hidden_size = 512\n",
    "\n",
    "# data fitting, training and accuracy evaluation\n",
    "model = NN(num_inputs,hidden_size,num_outputs)\n",
    "cost_dict, tests_dict = model.train(x_train,y_train,num_iterations=num_iterations,learning_rate=learning_rate)\n",
    "accu = model.testing(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
